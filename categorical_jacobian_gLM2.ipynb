{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TattaBio/gLM2/blob/main/categorical_jacobian_gLM2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD9_fkFDPUas"
      },
      "source": [
        "# Categorical Jacobian on gLM2\n",
        "Adapted from @sokrypton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TQDM_BAR_FORMAT = '{l_bar}{bar}| {n_fmt}/{total_fmt} [elapsed: {elapsed} remaining: {remaining}]'\n",
        "import os\n",
        "from typing import List\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import bokeh.plotting\n",
        "from bokeh.transform import linear_cmap\n",
        "from bokeh.plotting import figure, show\n",
        "from bokeh.palettes import viridis\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from matplotlib.colors import to_hex\n",
        "\n",
        "bokeh.io.output_notebook()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "fast = False # @param {type:\"boolean\"}\n",
        "#@markdown - only perturb the `mask` token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "cellView": "form",
        "id": "BwhOjguxaA0k"
      },
      "outputs": [],
      "source": [
        "#@markdown ## setup gLM2_650M\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "MODEL_NAME = \"tattabio/gLM2_650M\"\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "NUC_TOKENS = tuple(range(29, 33)) # 4 nucleotides a,t,c,g\n",
        "AA_TOKENS = tuple(range(4,24)) # 20 amino acids\n",
        "NUM_TOKENS = len(AA_TOKENS) + len(NUC_TOKENS)\n",
        "\n",
        "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME, trust_remote_code=True).eval().to(DEVICE)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "MASK_TOKEN_ID = tokenizer.mask_token_id\n",
        "\n",
        "def create_figure(contact_df, tokens: List[str]):\n",
        "  tools = \"hover,save,pan,box_zoom,reset,wheel_zoom\"\n",
        "  seqlen = len(tokens)\n",
        "  contact_df['i_token'] = contact_df['i'] + ': ' + contact_df['i'].astype(int).map(lambda x: tokens[x-1])\n",
        "  contact_df['j_token'] = contact_df['j'] + ': ' + contact_df['j'].astype(int).map(lambda x: tokens[x-1])\n",
        "  p = figure(title=\"COEVOLUTION\",\n",
        "            x_range=[str(x) for x in range(1,seqlen+1)],\n",
        "            y_range=[str(x) for x in range(1,seqlen+1)][::-1],\n",
        "            width=800, height=800,\n",
        "            tools=tools, toolbar_location='below',\n",
        "            tooltips=[('i', '@i_token'), ('j', '@j_token'), ('value', '@value')])\n",
        "\n",
        "  cmap = plt.colormaps[\"Blues\"]\n",
        "  blues = [to_hex(cmap(i)) for i in np.linspace(0, 1, 256)]\n",
        "  r = p.rect(x=\"i\", y=\"j\", width=1, height=1, source=contact_df,\n",
        "            fill_color=linear_cmap('value', blues, low=contact_df.value.min(), high=np.percentile(contact_df.value, 99)),\n",
        "            line_color=None)\n",
        "  p.xaxis.visible = False  # Hide the x-axis\n",
        "  p.yaxis.visible = False  # Hide the x-axis\n",
        "  return p\n",
        "\n",
        "\n",
        "def contact_to_dataframe(con):\n",
        "  sequence_length = con.shape[0]\n",
        "  idx = [str(i) for i in np.arange(1, sequence_length + 1)]\n",
        "  df = pd.DataFrame(con, index=idx, columns=idx)\n",
        "  df = df.stack().reset_index()\n",
        "  df.columns = ['i', 'j', 'value']\n",
        "  return df\n",
        "\n",
        "def jac_to_contact(jac, symm=True, center=True, diag=\"remove\", apc=True):\n",
        "\n",
        "  X = jac.copy()\n",
        "  Lx,Ax,Ly,Ay = X.shape\n",
        "\n",
        "  if center:\n",
        "    for i in range(4):\n",
        "      if X.shape[i] > 1:\n",
        "        X -= X.mean(i,keepdims=True)\n",
        "\n",
        "  contacts = np.sqrt(np.square(X).sum((1,3)))\n",
        "\n",
        "  if symm and (Ax != 20 or Ay != 20):\n",
        "    contacts = (contacts + contacts.T)/2\n",
        "\n",
        "  if diag == \"remove\":\n",
        "    np.fill_diagonal(contacts,0)\n",
        "\n",
        "  if diag == \"normalize\":\n",
        "    contacts_diag = np.diag(contacts)\n",
        "    contacts = contacts / np.sqrt(contacts_diag[:,None] * contacts_diag[None,:])\n",
        "\n",
        "  if apc:\n",
        "    ap = contacts.sum(0,keepdims=True) * contacts.sum(1, keepdims=True) / contacts.sum()\n",
        "    contacts = contacts - ap\n",
        "\n",
        "  if diag == \"remove\":\n",
        "    np.fill_diagonal(contacts,0)\n",
        "\n",
        "  return contacts\n",
        "\n",
        "\n",
        "def get_categorical_jacobian(sequence: str, fast: bool = False):\n",
        "  all_tokens = NUC_TOKENS + AA_TOKENS\n",
        "  num_tokens = len(all_tokens)\n",
        "\n",
        "  input_ids = torch.tensor(tokenizer.encode(sequence), dtype=torch.int)\n",
        "  tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "  seqlen = input_ids.shape[0]\n",
        "  # [seqlen, 1, seqlen, 1].\n",
        "  is_nuc_pos = torch.isin(input_ids, torch.tensor(NUC_TOKENS)).view(-1, 1, 1, 1).repeat(1, 1, seqlen, 1)\n",
        "  # [1, num_tokens, 1, num_tokens].\n",
        "  is_nuc_token = torch.isin(torch.tensor(all_tokens), torch.tensor(NUC_TOKENS)).view(1, -1, 1, 1).repeat(1, 1, 1, num_tokens)\n",
        "  # [seqlen, 1, seqlen, 1].\n",
        "  is_aa_pos = torch.isin(input_ids, torch.tensor(AA_TOKENS)).view(-1, 1, 1, 1).repeat(1, 1, seqlen, 1)\n",
        "  # [1, num_tokens, 1, num_tokens].\n",
        "  is_aa_token = torch.isin(torch.tensor(all_tokens), torch.tensor(AA_TOKENS)).view(1, -1, 1, 1).repeat(1, 1, 1, num_tokens)\n",
        "\n",
        "  input_ids = input_ids.unsqueeze(0).to(DEVICE)\n",
        "  with torch.no_grad(), torch.cuda.amp.autocast(enabled=True):\n",
        "    f = lambda x:model(x)[0][..., all_tokens].cpu().float()\n",
        "\n",
        "    x = torch.clone(input_ids).to(DEVICE)\n",
        "    ln = x.shape[1]\n",
        "\n",
        "    fx = f(x)[0]\n",
        "    if fast:\n",
        "      fx_h = torch.zeros((ln, 1 , ln, num_tokens), dtype=torch.float32)\n",
        "    else:\n",
        "      fx_h = torch.zeros((ln,num_tokens,ln,num_tokens),dtype=torch.float32)\n",
        "      x = torch.tile(x,[num_tokens,1])\n",
        "    with tqdm(total=ln, bar_format=TQDM_BAR_FORMAT) as pbar:\n",
        "      for n in range(ln): # for each position\n",
        "        x_h = torch.clone(x)\n",
        "        if fast:\n",
        "          x_h[:, n] = MASK_TOKEN_ID\n",
        "        else:\n",
        "          x_h[:, n] = torch.tensor(all_tokens)\n",
        "        fx_h[n] = f(x_h)\n",
        "        pbar.update(1)\n",
        "    jac = fx_h-fx\n",
        "    valid_nuc = is_nuc_pos & is_nuc_token\n",
        "    valid_aa = is_aa_pos & is_aa_token\n",
        "    # Zero out other modality\n",
        "    jac = torch.where(valid_nuc | valid_aa, jac, 0.0)\n",
        "    contact = jac_to_contact(jac.numpy())\n",
        "  return jac, contact, tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Protein-Protein Interaction Example (ModAC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MOD_A = \"MFLKVRAEKRLGNFRLNVDFEMGRDYCVLLGPTGAGKSVFLELIAGIVKPDRGEVRLNGADITPLPPERRGIGFVPQDYALFPHLSVYRNIAYGLRNVERVERDRRVREMAEKLGIAHLLDRKPARLSGGERQRVALARALVIQPRLLLLDEPLSAVDLKTKGVLMEELRFVQREFDVPILHVTHDLIEAAMLADEVAVMLNGRIVEKGKLKELFSAKNGEVAEFLSARNLLLKVSKILD\"\n",
        "MOD_C = \"MRLLFSALLALLSSIILLFVLLPVAATVTLQLFNFDEFLKAASDPAVWKVVLTTYYAALISTLIAVIFGTPLAYILARKSFPGKSVVEGIVDLPVVIPHTVAGIALLVVFGSSGLIGSFSPLKFVDALPGIVVAMLFVSVPIYINQAKEGFASVDVRLEHVARTLGSSPLRVFFTVSLPLSVRHIVAGAIMSWARGISEFGAVVVIAYYPMIAPTLIYERYLSEGLSAAMPVAAILILLSLAVFVALRIIVG\"\n",
        "\n",
        "sequence = f\"<+>{MOD_A}<+>{MOD_C}\"\n",
        "J, contact, tokens = get_categorical_jacobian(sequence, fast=fast)\n",
        "df = contact_to_dataframe(contact)\n",
        "p = create_figure(df, tokens)\n",
        "show(p)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## tRNA Structure\n",
        "NOTE: Nucleotides must be lowercase for gLM2!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRNA = \"ggagcggtagttcagtcggttagaatacctgcctgtcacgcagggggtcgcgggttcgagtcccgtccgttccgcca\"\n",
        "sequence = f\"<+>{TRNA}\"\n",
        "J, contact, tokens = get_categorical_jacobian(sequence, fast=fast)\n",
        "df = contact_to_dataframe(contact)\n",
        "p = create_figure(df, tokens)\n",
        "show(p)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".glm2-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
